<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Currency Detection</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color:rgb(240, 218, 178);
            color: rgb(136, 91, 91); 
        }
        
        h1 {
            margin: 20px 0;
        }
        
        .video-container {
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 10px;
        }
        
        img {
            border: 4px solid rgb(136, 91, 91);
            border-radius: 8px; 
        }
    </style>
</head>
<body>
    <h1>Currency Detection</h1>
    <div class="video-container">
        <video id="video" width="640" height="640" autoplay></video>
        <canvas id="canvas" width="640" height="640"></canvas>
    </div>

    <script src="wasmFeatureDetect.js"></script> <!-- Load wasmFeatureDetect.js -->
    <script>
        const video = document.getElementById("video");
        const canvas = document.getElementById("canvas");
        const ctx = canvas.getContext("2d");
        let model = null;
        let lastSpokenLabel = "";
        let lastSpokenTime = 0;
        const SPEAK_INTERVAL = 3000; // 2 seconds

        // Initialize WebAssembly and load model
        async function loadModel() {
            try {
                await wasmFeatureDetect.initWasm();  // Initialize WebAssembly with wasmFeatureDetect.js
                console.log("WebAssembly initialized!");

                // Assuming you have another function for model loading
                model = await loadCurrencyModel('best_model.param', 'best_model.bin'); // Load your model here
                console.log("Model loaded successfully!");

                // Start detecting after the model is loaded
                setInterval(detectCurrency, 1000);  // Detect every second
            } catch (error) {
                console.error("Error during WebAssembly initialization or model loading:", error);
            }
        }

        // Function to load your model, adjust for your use case
        async function loadCurrencyModel(paramFile, binFile) {
            // Replace this with your actual model loading logic
            return { detect: function(inputTensor) { 
                console.log("Detection function called");
                return []; // Return mock detection results
            }};
        }

        async function detectCurrency() {
            // Resize canvas to 640x640 before drawing the image
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            console.log("Image Data:", imageData);

            // Convert image data to tensor for NCNN or your model
            const inputTensor = new ncnn.Tensor(imageData.data, [1, 3, 640, 640]);  // Adjust for model
            console.log("Tensor created:", inputTensor);

            // Run model
            let results;
            try {
                results = await model.detect(inputTensor); // Call detect function from loaded model
                console.log("Detection Results:", results);
            } catch (error) {
                console.error("Error during detection:", error);
                return;
            }

            // Process results
            processResults(results);
        }

        function processResults(results) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            let detectedLabel = null;
            if (results && results.length > 0) {
                results.forEach((box) => {
                    const [x, y, w, h, conf, classId] = box;
                    if (conf > 0.5) {
                        detectedLabel = `Currency ${classId}`;
                        ctx.strokeStyle = "red";
                        ctx.lineWidth = 2;
                        ctx.strokeRect(x, y, w, h);
                        ctx.fillStyle = "red";
                        ctx.fillText(detectedLabel, x, y - 5);
                    }
                });
            }

            // Speak detected label with a delay to avoid repeating
            if (detectedLabel && shouldSpeak(detectedLabel)) {
                speak(detectedLabel);
            }
        }

        function shouldSpeak(label) {
            const currentTime = Date.now();
            if (label !== lastSpokenLabel || currentTime - lastSpokenTime > SPEAK_INTERVAL) {
                lastSpokenLabel = label;
                lastSpokenTime = currentTime;
                return true;
            }
            return false;
        }

        function speak(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            speechSynthesis.speak(utterance);
            console.log(`Spoken: ${text}`);
        }

        // Start capturing video from the webcam
        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 640 } });
                video.srcObject = stream;
            } catch (error) {
                console.error("Error accessing webcam:", error);
            }
        }

        // Initialize everything
        async function init() {
            setupCamera();
            loadModel();  // Load the model after setting up the camera
        }

        init();
    </script>
</body>
</html>
