<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.jsdelivr.net/gh/nihui/ncnn-webassembly-yolov5@ncnn.js"></script>
    <title>Currency Detection</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
        }
        h1 {
            margin: 20px 0;
        }
        .video-container {
            position: relative;
            width: 640px;
            height: 480px;
        }
        video, canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>
<body>
    <h1>Currency Detection</h1>
    <div class="video-container">
        <video id="video" width="640" height="480" autoplay></video>
        <canvas id="canvas" width="640" height="480"></canvas>
    </div>

    <!-- Link to NCNN WebAssembly -->
    <script src="https://cdn.jsdelivr.net/npm/ncnn-wasm@1.0.0/ncnn.js"></script>

    <script>
        // Initialize the module with the correct export name
        var createEchoCash = Module({
            // You can pass any necessary configurations here if needed
        });

        const video = document.getElementById("video");
        const canvas = document.getElementById("canvas");
        const ctx = canvas.getContext("2d");
        let lastSpokenLabel = "";
        let lastSpokenTime = 0;
        const SPEAK_INTERVAL = 2000; // 2 seconds

        // Initialize NCNN model
        async function loadModel() {
            // Initialize the WebAssembly version of NCNN
            await ncnn.initWasm();  // Initialize WebAssembly

            // Load the model with .param and .bin files
            const model = await ncnn.loadModel('best_model.param', 'best_model.bin');
            console.log("NCNN model loaded!");

            async function detectCurrency() {
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                
                // Convert image data to tensor for NCNN
                const inputTensor = new ncnn.Tensor(imageData.data, [1, 3, 640, 480]);

                // Run model
                const results = await model.detect(inputTensor);

                // Process results
                processResults(results);
            }

            function processResults(results) {
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                let detectedLabel = null;
                if (results && results.length > 0) {
                    results.forEach((box) => {
                        const [x, y, w, h, conf, classId] = box;
                        if (conf > 0.5) {
                            detectedLabel = `Currency ${classId}`;
                            ctx.strokeStyle = "red";
                            ctx.lineWidth = 2;
                            ctx.strokeRect(x, y, w, h);
                            ctx.fillStyle = "red";
                            ctx.fillText(detectedLabel, x, y - 5);
                        }
                    });
                }

                // Speak detected label with a delay to avoid repeating
                if (detectedLabel && shouldSpeak(detectedLabel)) {
                    speak(detectedLabel);
                }
            }

            function shouldSpeak(label) {
                const currentTime = Date.now();
                if (label !== lastSpokenLabel || currentTime - lastSpokenTime > SPEAK_INTERVAL) {
                    lastSpokenLabel = label;
                    lastSpokenTime = currentTime;
                    return true;
                }
                return false;
            }

            function speak(text) {
                const utterance = new SpeechSynthesisUtterance(text);
                speechSynthesis.speak(utterance);
                console.log(`Spoken: ${text}`);
            }

            setInterval(detectCurrency, 1000);
        }

        // Start capturing video from the webcam
        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
            } catch (error) {
                console.error("Error accessing webcam:", error);
            }
        }

        setupCamera();
        loadModel();
    </script>
</body>
</html>
