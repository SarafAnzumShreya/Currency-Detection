<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Currency Detection</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: rgb(240, 218, 178);
            color: rgb(136, 91, 91);
        }
        
        h1 {
            margin: 20px 0;
        }
        
        .video-container {
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 10px;
        }
        
        img {
            border: 4px solid rgb(136, 91, 91);
            border-radius: 8px;
        }
    </style>
</head>
<body>
    <h1>Currency Detection</h1>
    <div class="video-container">
        <video id="video" width="640" height="640" autoplay></video>
        <canvas id="canvas" width="640" height="640"></canvas>
    </div>
    
    <script src="echocash.js"></script> <!-- Load the JavaScript file that loads the WASM model -->

    <script>
        const video = document.getElementById("video");
        const canvas = document.getElementById("canvas");
        const ctx = canvas.getContext("2d");
        let model = null;
        let lastSpokenLabel = "";
        let lastSpokenTime = 0;
        const SPEAK_INTERVAL = 3000; // 3 seconds between speaking

        // Initialize NCNN WebAssembly
        async function loadModel() {
            try {
                // Load the model files (adjust paths if necessary)
                model = await ncnn.loadModel('echocash.param', 'echocash.bin');
                console.log("NCNN model loaded!");

                // Start detecting once the model is loaded
                setInterval(detectCurrency, 1000);
            } catch (error) {
                console.error("Error loading model:", error);
                return;
            }
        }

        // Set up video capture from webcam
        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 640 } });
                video.srcObject = stream;
            } catch (error) {
                console.error("Error accessing webcam:", error);
            }
        }

        // Detect currency in the video stream
        async function detectCurrency() {
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

            const inputTensor = new ncnn.Tensor(imageData.data, [1, 3, 640, 640]);

            // Run model detection
            let results = await model.detect(inputTensor);

            // Process results
            processResults(results);
        }

        // Process and draw detection results
        function processResults(results) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            let detectedLabel = null;
            if (results && results.length > 0) {
                results.forEach((box) => {
                    const [x, y, w, h, conf, classId] = box;
                    if (conf > 0.5) {
                        detectedLabel = `Currency ${classId}`;
                        ctx.strokeStyle = "red";
                        ctx.lineWidth = 2;
                        ctx.strokeRect(x, y, w, h);
                        ctx.fillStyle = "red";
                        ctx.fillText(detectedLabel, x, y - 5);
                    }
                });
            }

            // Speak detected label with a delay
            if (detectedLabel && shouldSpeak(detectedLabel)) {
                speak(detectedLabel);
            }
        }

        // Check if the detected label should be spoken
        function shouldSpeak(label) {
            const currentTime = Date.now();
            if (label !== lastSpokenLabel || currentTime - lastSpokenTime > SPEAK_INTERVAL) {
                lastSpokenLabel = label;
                lastSpokenTime = currentTime;
                return true;
            }
            return false;
        }

        // Use speech synthesis to speak the label
        function speak(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            speechSynthesis.speak(utterance);
            console.log(`Spoken: ${text}`);
        }

        // Initialize the video capture and model loading
        async function init() {
            setupCamera();
            loadModel();
        }

        init();
    </script>
</body>
</html>
